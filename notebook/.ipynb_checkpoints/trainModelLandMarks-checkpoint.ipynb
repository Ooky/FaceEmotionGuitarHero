{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We are going to train a convolutional neural network (cnn) to recognize facial emotions. Then we use the output of the emotions to play a game in a browser which is like guitarhero. Instead of pressing the matching button at the right time, you can use your face and make the same emotion as needed. Our program then checks wheter you did a good job (same emotion) or if you weren't that clear with your facial emotion!\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "As a dataset, we use videos from 23 different actors who sing and say a sentence in eight different emotions. \n",
    "Our old approach was the following: We will seperate the videos into each emotion and then take several screenshots from the video.\n",
    "Our new approach is to use a face landmark recognition api too feed our model with our data to train it on different emotions.\n",
    "\n",
    "\n",
    "## Credits\n",
    "\n",
    "\n",
    "Credits:\n",
    "\"[The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)](https://zenodo.org/record/1188976)\" by Livingstone & Russo is licensed under [CC BY-NA-SC 4.0.](https://creativecommons.org/licenses/by-nc-sa/4.0/)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programme\\Anaconda3\\envs\\compx\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "First of all we inport os, shutil and glob. glob is used to match path names.\n",
    "We are going to copy every video into one folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of instances:  35888\n"
     ]
    }
   ],
   "source": [
    "with open(\"D:/Arbeit/semester5/comppx18/Projekt/Datasets/KaggleEmotion/fer2013/fer2013.csv\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "    lines = np.array(content)\n",
    "\n",
    "    num_of_instances = lines.size\n",
    "print(\"number of instances: \",num_of_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__(): incompatible function arguments. The following argument types are supported:\n    1. (self: dlib.fhog_object_detector, image: array, upsample_num_times: int=0) -> dlib.rectangles\n\nInvoked with: <dlib.fhog_object_detector object at 0x000001C1D042A4C8>, <PIL.Image.Image image mode=RGB size=4x2304 at 0x1C1D0420F98>, 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-cd2a42677b00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgist_earth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpixels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mpixels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_recognition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mface_landmarks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpixels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda3\\envs\\compx\\lib\\site-packages\\face_recognition\\api.py\u001b[0m in \u001b[0;36mface_landmarks\u001b[1;34m(face_image, face_locations, model)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdicts\u001b[0m \u001b[0mof\u001b[0m \u001b[0mface\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0mlocations\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0meyes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \"\"\"\n\u001b[1;32m--> 174\u001b[1;33m     \u001b[0mlandmarks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_raw_face_landmarks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mface_locations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m     \u001b[0mlandmarks_as_tuples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlandmark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlandmark\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlandmarks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda3\\envs\\compx\\lib\\site-packages\\face_recognition\\api.py\u001b[0m in \u001b[0;36m_raw_face_landmarks\u001b[1;34m(face_image, face_locations, model)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raw_face_landmarks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mface_locations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"large\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mface_locations\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mface_locations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_raw_face_locations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mface_locations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_css_to_rect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_location\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mface_location\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mface_locations\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda3\\envs\\compx\\lib\\site-packages\\face_recognition\\api.py\u001b[0m in \u001b[0;36m_raw_face_locations\u001b[1;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcnn_face_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __call__(): incompatible function arguments. The following argument types are supported:\n    1. (self: dlib.fhog_object_detector, image: array, upsample_num_times: int=0) -> dlib.rectangles\n\nInvoked with: <dlib.fhog_object_detector object at 0x000001C1D042A4C8>, <PIL.Image.Image image mode=RGB size=4x2304 at 0x1C1D0420F98>, 1"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "import time\n",
    "x_train, y_train, x_test, y_test = [], [], [], []\n",
    "for i in range(1,num_of_instances):\n",
    "    emotion, img, usage = lines[i].split(\",\")\n",
    "\n",
    "    val = img.split(\" \")\n",
    "    pixels = np.array(val, 'float32')\n",
    "    emotion = keras.utils.to_categorical(emotion, 7)\n",
    "    #pixels = Image.fromarray(np.uint8(cm.gist_earth(pixels)*255))\n",
    "    #pixels = pixels.save('test.png')\n",
    "    im = Image.fromarray(np.uint8(cm.gist_earth(pixels)*255))\n",
    "    pixels = im.convert('RGB')\n",
    "    print(face_recognition.face_landmarks(pixels))\n",
    "    break\n",
    "    try:\n",
    "        emotion, img, usage = lines[i].split(\",\")\n",
    "\n",
    "        val = img.split(\" \")\n",
    "        pixels = np.array(val, 'float32')\n",
    "        emotion = keras.utils.to_categorical(emotion, 7)\n",
    "        #pixels = Image.fromarray(np.uint8(cm.gist_earth(pixels)*255))\n",
    "        #pixels = pixels.save('test.png')\n",
    "        im = Image.fromarray(np.uint8(cm.gist_earth(pixels)*255))\n",
    "        im.convert('RGB')\n",
    "        time.sleep(0.5)\n",
    "        break\n",
    "        print(face_recognition.face_landmarks(face_recognition.load_image_file(pixels)))\n",
    "        break\n",
    "        if 'Training' in usage:\n",
    "            y_train.append(emotion)\n",
    "            print(face_recognition.face_landmarks(pixels))\n",
    "            x_train.append(face_recognition.face_landmarks(pixels))\n",
    "        elif 'PublicTest' in usage:\n",
    "            y_test.append(emotion)\n",
    "            x_test.append(pixels)\n",
    "    except:\n",
    "        print(\"\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "#x_train = np.array(x_train)\n",
    "y_test = np.array(y_test)\n",
    "#x_test = np.array(x_test)\n",
    "#x_train = x_train.reshape((len(x_train),48,48)+(1,))\n",
    "#x_test = x_test.reshape((len(x_test),48,48)+(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#np.concatenate((np.ones((len(x_train),1), dtype=np.int), x_train.reshape(len(x_train),1)), axis=1)\n",
    "#print(x_train.shape)\n",
    "#x_train = np.insert(x_train, 0, values=30, axis=1) \n",
    "#print(x_train)\n",
    "#print(x_train.shape)\n",
    "print(x_train)\n",
    "#x_train = x_train.reshape((len(x_train), len(x_train[0])))\n",
    "#x_train = x_train.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 70.]\n",
      "  [ 80.]\n",
      "  [ 82.]\n",
      "  ...\n",
      "  [ 52.]\n",
      "  [ 43.]\n",
      "  [ 41.]]\n",
      "\n",
      " [[ 65.]\n",
      "  [ 61.]\n",
      "  [ 58.]\n",
      "  ...\n",
      "  [ 56.]\n",
      "  [ 52.]\n",
      "  [ 44.]]\n",
      "\n",
      " [[ 50.]\n",
      "  [ 43.]\n",
      "  [ 54.]\n",
      "  ...\n",
      "  [ 49.]\n",
      "  [ 56.]\n",
      "  [ 47.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 91.]\n",
      "  [ 65.]\n",
      "  [ 42.]\n",
      "  ...\n",
      "  [ 72.]\n",
      "  [ 56.]\n",
      "  [ 43.]]\n",
      "\n",
      " [[ 77.]\n",
      "  [ 82.]\n",
      "  [ 79.]\n",
      "  ...\n",
      "  [105.]\n",
      "  [ 70.]\n",
      "  [ 46.]]\n",
      "\n",
      " [[ 77.]\n",
      "  [ 72.]\n",
      "  [ 84.]\n",
      "  ...\n",
      "  [106.]\n",
      "  [109.]\n",
      "  [ 82.]]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported image type, must be 8bit gray or RGB image.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-23d975b4f473>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mface_landmarks_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mface_landmarks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_landmarks_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda3\\envs\\compx\\lib\\site-packages\\face_recognition\\api.py\u001b[0m in \u001b[0;36mface_landmarks\u001b[1;34m(face_image, face_locations, model)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdicts\u001b[0m \u001b[0mof\u001b[0m \u001b[0mface\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0mlocations\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0meyes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \"\"\"\n\u001b[1;32m--> 174\u001b[1;33m     \u001b[0mlandmarks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_raw_face_landmarks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mface_locations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m     \u001b[0mlandmarks_as_tuples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlandmark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlandmark\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlandmarks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda3\\envs\\compx\\lib\\site-packages\\face_recognition\\api.py\u001b[0m in \u001b[0;36m_raw_face_landmarks\u001b[1;34m(face_image, face_locations, model)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raw_face_landmarks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mface_locations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"large\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mface_locations\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mface_locations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_raw_face_locations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mface_locations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_css_to_rect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_location\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mface_location\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mface_locations\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda3\\envs\\compx\\lib\\site-packages\\face_recognition\\api.py\u001b[0m in \u001b[0;36m_raw_face_locations\u001b[1;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcnn_face_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unsupported image type, must be 8bit gray or RGB image."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    " \n",
    "#1st convolution layer\n",
    "model.add(Conv2D(64, (5, 5), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(MaxPooling2D(pool_size=(5,5), strides=(2, 2)))\n",
    " \n",
    "#2nd convolution layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n",
    " \n",
    "#3rd convolution layer\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n",
    " \n",
    "model.add(layers.Flatten())\n",
    " \n",
    "#fully connected neural networks\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    " \n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 32/128 [======>.......................] - ETA: 33:40 - loss: 13.5400 - acc: 0.1343"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-259-27d9d4dd33a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Programme\\Anaconda3\\envs\\compx\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda3\\envs\\compx\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda3\\envs\\compx\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda3\\envs\\compx\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda3\\envs\\compx\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda3\\envs\\compx\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda3\\envs\\compx\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "batch_size = 128\n",
    "gen = ImageDataGenerator()\n",
    "train_generator = gen.flow(x_train, y_train, batch_size=batch_size)\n",
    " \n",
    "model.compile(loss='categorical_crossentropy'\n",
    ", optimizer=keras.optimizers.Adam()\n",
    ", metrics=['accuracy']\n",
    ")\n",
    "model.fit_generator(train_generator, steps_per_epoch=batch_size, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"faceEmotion3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"models/faceEmotion2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03454698033878828\n",
      "Train accuracy: 98.92368246891219\n",
      "Test loss: 3.4292131838741393\n",
      "Test accuracy: 55.28002229116195\n"
     ]
    }
   ],
   "source": [
    "train_score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train loss:', train_score[0])\n",
    "print('Train accuracy:', 100*train_score[1])\n",
    " \n",
    "test_score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', test_score[0])\n",
    "print('Test accuracy:', 100*test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn.metrics import classification_report, confusion_matrix\n",
    "pred_list = []; actual_list = []\n",
    "for i in predictions:\n",
    "pred_list.append(np.argmax(i))\n",
    "for i in y_test:\n",
    "actual_list.append(np.argmax(i))\n",
    "confusion_matrix(actual_list, pred_list)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def emotion_analysis(emotions):\n",
    "    objects = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "    y_pos = np.arange(len(objects))\n",
    "\n",
    "    plt.bar(y_pos, emotions, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, objects)\n",
    "    plt.ylabel('percentage')\n",
    "    plt.title('emotion')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJztnWusX8WV5df2xeFp8BPjB37wSGInyhDFBqRMEuLuRHRoNVEUIkKrxUhI5MMkStNEHTIjobQ0IyVfOv0hox6hCWpDEKRfEYj0KDJMoEXUwjGOIXY7YGNsMDZ+PxMDftR8uH9a96xa9n9zbf/vtWr9JMuucp06VXVO+dy9vPeuKKXAGNMWE8Z6AMaYweONb0yDeOMb0yDe+MY0iDe+MQ3ijW9Mg3jjG9Mg3vjGNMhpbfyIuDkiXo6IjRFx35kalDHm7BKj9dyLiCEArwD4HICtAH4F4KullH8/2TUXX3xxmTp1aqcuc/+I6Nvm2LFjnfLRo0erNsePH++Uh4aGqjZHjhzplE+cOFG1mTCh+++lmoO6jueh5sVj4jGr60b7DHke6l6Z6zJrxGWgHre6P1933nnnVW342at1Vdf1G4+qU/3w/dR7NWnSpFOWgfqdVet68cUXn3J8W7Zswe7du/tumP6rcXKuB7CxlLIJACLiMQC3Ajjpxp86dSruueeeTh0/NLX4aiGZPXv2dMrbtm2r2hw6dKhTvuyyy6o2L730Uqf87rvvVm0uuOCCTpnnAAC///3vqzp+aT7wgQ9UbS655JJO+fDhw337UffndVTretFFF/W9l+qbr1NrxHPjeam+9+7dW7Xh66ZPn1612b17d6esNue0adM6ZfUPkZoH/2M0efLkqs3EiRM7Zf64AcBNN93UKX/2s5+t2mzfvr1TVu/Q0qVLO2VewxtvvLG6RnE6P+rPAfDGiPLWXp0xZpxzOhtf/ThRfVYi4u6IWBURq373u9+dxu2MMWeK09n4WwFcOaI8F0D183Up5YFSypJSyhK2T4wxY8Pp2Pi/AnBtRCwE8CaA2wHccaoLIqKy/bis7DO2vdRPDvyPitIF2F7cvHlz1YYFFWWHs1319ttv9+0HqG1BZT+zna1sUZ5bxsZnXQKo11XpAMo25/mqvvk6NcYDBw50yqwdALW9rHQIvr/6wLAAd/DgwaqNWmt+/mquLNQp23z16tWd8pe+9KWqDc81I2rzO5W5BjiNjV9KORYRXwfwcwBDAB4spawbbX/GmMFxOl98lFL+BcC/nKGxGGMGhD33jGmQ0/riv18mTJhQ2XFsC2ecWvj/7AFg165d1b0YtjvZWQeo7UNlv2dQ9irPTekHatzMaJyM2BYE6rmptVeOUBldhvUC1c/555/fKV966aVVG/5/dLWu3I/6f3TWGKZMmVK1yTj5KP1ArS3DY1RrnbXPzwT+4hvTIN74xjSIN74xDeKNb0yDDFTcGxoaqpwdWIT6/Oc/X133yCOPdMqZYArl6DEaoU45tezfv79TVk4uGaFIiXsMi0JAPTfVhu/PYwbquV144YVVG+V4w2KWuo5RTlcs1Kl7seClhEyeBwftAPU7k4kWVPefMWNG1YbXWr0P7NTz05/+tGqjnHrOFv7iG9Mg3vjGNIg3vjENMlAbf8qUKbjttts6dZnsMW+++WanrDK1cD/KqWLfvn2dsgrk4X6UTTlaMtlkuE45dXAyCOWIxHa/asO2qNJOMklQVN9cp4JbGGXj8/0zgTTKWYjXMZttiNdR6UTs1KPWkftZu3Zt1Yaf67Jly6o2/H6O1unHX3xjGsQb35gG8cY3pkG88Y1pkIGKe4cOHcKzzz7bqVu4cGGnrLLjzp07t1N+7bXXqjYs3CkxiYW6TAZdlU2FRTHlQKPIpOXmvpS4yIKXEgl53KofFtOUkKdEUp6HWiOeR8ZZSd2L10j1w3NTwh0/VyUAZkRBJe7xXNXzYKcrlS2Y23BWaKB2luK5ZlOt+4tvTIN44xvTIN74xjTIQG38SZMm4TOf+cwp28ybN6+qe+qpp/r2vXHjxk75Ix/5SNWGbXoVTMFkHHiUbaqcUdg+VddlgnvY9lNBMmwvqswxGWcQZTMqm74fyvGGx63mzuufcSjKPI+M8xZQj3vnzp1VG15btT58atOWLVuqNh/84Ac75TfeeKNq008neuedd6prFP7iG9Mg3vjGNIg3vjEN4o1vTIMMVNxTZKLhOGppx44dVZtZs2Z1ykooyjiRsAik+uHMMUooUmIWO3ooJyMeoxKcWExSjifcJnPMlhqzeh68JmqNeG5KOOS5qvtnjlFnJ5ZMxJpa+8waqTYsqHGWKaB+Riq9N4vPKotUv/Tr2WhSf/GNaRBvfGMaxBvfmAYZcxuf+fGPf1zVcdZUlbE1c3Q1O1bMnDmzasPZaDPHXCkbX9WxNqAcb9j5RAWcsL2obNqM3cv2aya4BahtUbVGmePCMrZ45kixjHbDtm9GgwFyOgg/M2Vn87unHKpYK1HPg9cs49Ck8BffmAbxxjemQbzxjWkQb3xjGmSg4l4ppXJ24GgnlXb48ssv75QPHjxYtdm7d2+nrM5RZ0FFiYQszKgz2zNnnSvhjoUyNUYWJTNHWGUdbxiefyYDDlDPLZO5Rwlnmcw53LcaI69/5ngs1UY9RxZSOcpOXafeKx737NmzqzYcdanGyHUZ8VPhL74xDeKNb0yD9N34EfFgROyMiLUj6qZGxIqI2ND7vXY8NsaMWzI2/t8B+CGAh0bU3Qfg6VLK9yLivl752/06OnLkCNavX9+p4ywjymb52c9+1ilnjmVWGXQz9ju3UXZ4JsutcnzJBCTx3JSNz3anasN1yn7leahgH2V3Kzu7X9+KjN2dIXOvTGBRhsw7ozLw8HNVgWa33HJLp7xo0aKqTb81Uu+rou9Kl1L+FcBeqr4VwPLen5cD+GLqbsaYccFobfyZpZTtAND7/fI+7Y0x44izLu5FxN0RsSoiVrEfvDFmbBjtxt8REbMAoPd7nXq0RynlgVLKklLKEk6oYYwZG0brwPMEgDsBfK/3++OZi44fP14ddcWON0qUY0FDCU58Hrvqh8UrleGEBScl5nBa7ozYBeQixFRUIXOmIrQYNR4l+GVEuMxxYZlxj2Zu6prROPkAteORetbseJPJEqSe84c+9KG+Y8xENGbI/HfeowD+DcCHImJrRNyF4Q3/uYjYAOBzvbIx5hyh7xe/lPLVk/zVH5zhsRhjBoQ994xpkIEG6Zw4caKyxV9//fVOWdk+bFMr25xtnz179lRtOMBCOXEsWLCgU1b2IgcaZe2uTFAKB3hkjj3OtMk48GSddTJHb3GdGmNG8+AxqeehHJj6jScT7APU74h6P3keaq4Z7Yb3htKXssdg98NffGMaxBvfmAbxxjemQbzxjWmQMU+vzQ4iyq2XRRclQnGdOid8/vz5nbLKpsICk8qmwmQi6AA9NyZ7vvlIlEjJ4lXGEUZFCyrBK+Ocw6g23HfmCKszdYSWQr1X/I6oyFCOtFPOY7z+6l347ne/2ymzYxBQZ5HiZ8HRrifDX3xjGsQb35gG8cY3pkG88Y1pkDH33OM0RUrcYkEnk1ZLCVWcDkudX8bj4fECtVCjPAmVKMhpwZWnWsabjsU85YXG12XOrsuKYizCZaL11PPg56juPxovSSWs8hpl0pYD9TqyFylQn8G4ZcuWvmNU6bkmTZrUKXMkK6BTy49EvS8Kf/GNaRBvfGMaxBvfmAYZqI0/ceLE6jisTGri0aRdVk4tnHpY2Z1sryuHEW6jdAkVjcXzUPfnusy59hmnltGmyVZ9Zxx42IZWbbhO2ea8HpnMSmqu3E/m/QCAKVO6R0You5udfNT9uW+lAfE8OFIUAF588cVOeenSpZ2ySuuu8BffmAbxxjemQbzxjWkQb3xjGmSg4t67775bOTewEJJJ9ZSJRlPiCTtIKCGRUYLT9u3bO2Xl1KEcRFiYykTiZRx4lCMSizwqOi9z9vxoyThd8RqptWaUIMr9qHmwIKmi7JSIzONWfbOTl3o/WaTNrMfKlSurNpyOq1+03snwF9+YBvHGN6ZBvPGNaZCBB+mwYwvbJFdccUV1HWc4ydhQypGB7UMVXMN9sz0PAHPnzu2UX3vttaqNyrDCtqCyu/nIJqUf8DyUVsDrodaM9QPlwDLazD1MxmFFrRnbwmoe/A5lUnArG1sFMrFNrxyq+JmpdeQxKlucn+NXvvKVqs2BAwequtHgL74xDeKNb0yDeOMb0yDe+MY0yEDFvaNHj2Lbtm2dOhaGMmebZ0QxJdSwmKUixjJOPb/5zW865Y997GNVm7Vr11Z1LChlotqUmMVpl5WYxCKq6ofrMqm0VV3GqUaliuYxZrL07N69u2rDz1qNefLkyZ2yes7z5s2r6vh+M2bMqNqwKKeeK4uE6v28/fbbO+Xrr7++asN981wfeuih6hqFv/jGNIg3vjEN4o1vTIMM1MYvpVQ2Cts+KnMNB9coG58dG/gaoLarlG3MWgFnYAFqBx41Zs68CtQOPCqQh21IleWXtQqVFYbtVRUAk3Fq4fUA+tuZqm81V9YY3nrrrarN1KlTO2XlwMKZZznLEwBMnz69U1Y2fkY/UFoJO4upZ7Zo0aJO+be//W3V5oYbbuiUlebCdZnjyxT+4hvTIN74xjSIN74xDdJ340fElRHxi4hYHxHrIuKbvfqpEbEiIjb0fq+NYWPMuCQj7h0DcG8pZXVETALwQkSsAPBfADxdSvleRNwH4D4A3+7XGTtpsDCjotEyUW0ssKjoPBZGlHDFIpgSgTL9sJikxqSi6jJpqTnrinJ84XmozDFKqMrA81fCIQt+ymGFxUR2sgFqIVM50PC91Lw4EpNTYp+sb/Vs+6FEuW9961ud8te+9rWqDb8P6h3m94HFzjN2hFYpZXspZXXvz4cArAcwB8CtAJb3mi0H8MXUHY0xY877svEjYgGAjwN4HsDMUsp2YPgfBwD1/6EMX3N3RKyKiFUZd1hjzNknvfEj4hIA/wTgz0sppz6ycwSllAdKKUtKKUvU/5sbYwZPysiLiIkY3vSPlFL+uVe9IyJmlVK2R8QsADszfbE9dtVVV/G9qmtYB1DOIJmjp7iNsodYP1D2IttiO3fWU884VijbmPtW989kx2UdIJO1KJuhle+XOZ5LZQLOBBLxOqrnyh8U1Q/b6pkMTUD9rimHLr7frFmzqjZf//rXO2V+7wHg6aefPuW9gVoXYV0i+1N1RtUPAD8CsL6U8tcj/uoJAHf2/nwngMdTdzTGjDmZL/4nAfwZgN9ExJpe3X8D8D0Afx8RdwF4HcBtZ2eIxpgzTd+NX0p5DkD989Uwf3Bmh2OMGQT23DOmQQYanTdt2jTccccdnbqXX365U1aC1wsvvNApZ1IcL168uGrDYpJyzmChKnP0lHL8UBF7LFSp/+XgNkokzKQS5zZKuMo42ajrMkIq16loSX7Was0YJdzxuJXYyc9MCZlKpOR2aowsXKp1nDNnTqesntmaNWs65Uza8s2bN3fKKo25wl98YxrEG9+YBvHGN6ZBBmrjn3/++bjmmms6dVxWdtbPf/7zTlk5zLA9pGzKTMYZRtmvbGcqRwt1Hdvro81uw9epNcv0w7ZoJsNxFr5O2d18VHVGB1DZbTLPka/j46YB7WTEa6SeWSbL7vz58ztlzsgDAE899VSn/I1vfKNqw/fnuT/88MPVNQp/8Y1pEG98YxrEG9+YBvHGN6ZBBiruHT58GL/85S87dSxMqaw0LPoocY/bqFTJLDApMYkdNJTDRubs+YzzhYKvU32zmKhEucy57iz4ZVJgK5SYxdep9WAnFnV/bqOcnjJrzePJpqXOOFTxWivnnNdff/2UZaCemzoOix101q1b17dfhb/4xjSIN74xDeKNb0yDeOMb0yADFfeOHz+O/fv3d+r27t3bKaszxViYUim4+fw45WHFXljK4y2TOpsFv0y6b6AWr5TgpqIT+41ReeWpMTGZ6DwlZrG3WMZLUY2HBbdMlJ+C55HpR3kSqut43BnvRn6ngfqd3bRpU9Ums2aZ8WTwF9+YBvHGN6ZBvPGNaZCB2vh79uypnBI4HfA999xTXffkk092ynw+PZCLNOM65ZzDDkRKB+A61SaTKjqTOls5p/B1ysbnuszRSkpfUE41yj5m+H5KKxiNfaquydyLyTgdqTr1PDjl9qc+9amqzYoVKzpl5TzGx65l5sHPOfNsAH/xjWkSb3xjGsQb35gG8cY3pkEGnnrr6quv7tSx8wOnHwLqFE0qhTC3UWeIZcQkFm+UuMV1SvBR9+d2ajzqfkzmPDnljNOvn4zYqO6nIiqz5/CNJJPCLJMCO3tGPKPEzYx4tmPHjr5t+NmrM/gyKcD7jS+bKs1ffGMaxBvfmAbxxjemQQZq40+YMKHKMsL2urLz2NHm0KFDVRu28ZWtw7afsunY7lW2OrfJHDOl6vbs2VO1YZtNBQnxGqqAoMwYR5s6u9+9gHpt1f0zWYL42atnxtqEWjO+vxpzRj9QfPrTn+6UJ0+e3Lcfld6br8vcm+dhBx5jzEnxxjemQbzxjWkQb3xjGmSg4t7Ro0er1NgZQYMFi4zgNnXq1KoNCzxKzGFxkUVDICdIKicjdnTJnGeXSTmdYTRC0cnun4FFuMx59MpZiM+ze+utt6o2nBFInYHHdRmxUfWtBEiOqnviiSeqNixSqnnMmDGj7714HUfj9AP4i29Mk3jjG9MgfTd+RFwQESsj4sWIWBcRf9WrXxgRz0fEhoj4SUT0dw43xowLMjb+OwCWlVIOR8REAM9FxP8F8BcAflBKeSwi/jeAuwD87ak6mj17Nu6///5OHds6W7Zsqa5jO0vZ+OwQsX79+qoNH6uVsYeULcjjUfZ85sz2jLNFJgNOxoEm46yTPfYr41TD91Nt2MZXQULcj5or6yLKwevIkSOd8mWXXdb3XmpM6p3JHDPGGXfUGDPBRXwvvib7DPu++WWYw73ixN6vAmAZgH/s1S8H8MXUHY0xY07Kxo+IoYhYA2AngBUAXgWwv5Ty3udoK4A5Z2eIxpgzTWrjl1KOl1KuAzAXwPUAFqlm6tqIuDsiVkXEKuWbbowZPO9L1S+l7AfwDIAbAUyOiPcMjrkAtp3kmgdKKUtKKUumTZt2OmM1xpwh+qoSETEDwNFSyv6IuBDAHwL4PoBfAPgygMcA3Ang8X59DQ0NVSLcrl27OuXDhw+DGc2Z8YsXL67aPPvss52yEtdYBMpE0CmhRvWdiUbjNsqphddMRXqxuKgi1lgoUwKcEilZUBqtkMn9qDaZ6DMWG5UAx3NTY86MMZPuXKXXZpSTD797GfF3tBGWGVV/FoDlETGE4Z8Q/r6U8mRE/DuAxyLifwD4NYAfjWoExpiB03fjl1JeAvBxUb8Jw/a+MeYcw557xjTIQIN0jh07Vh2Tfc0113TKH/7wh/v2kzmW+aWXXqra8FHFPBYAOHDgQKecsbFVllmlQ/D9RxsUwnamOpY5k6Un4wikYHs5kxVHOafwPNjGBer1yDioKPudx6icrubPn1/V8fpnMiup58HvkVprfh4qU3K/rL8O0jHGnBRvfGMaxBvfmAbxxjemQQYq7h0+fBjPPfdcdwAkAimhKOPAwyLMq6++WrWZN29ep6ycc/bt29cpqww8PMZMdBYAHDx4sFNWTjUsFCpxkZ2clEiY6YfFIzUeBQt1SrjLZNfhNtu3b6/a8NoqZyV24Mkce6bEPY7eBIDLL7+8qmMef7zru8biLwBs3bq1U7722murNiw2q3ur/TGSrEDrL74xDeKNb0yDeOMb0yADtfHfeecdbNq0qVPHNglnKgFqO4+zkQK1442y3994441OWdnvGfg6ZeMqmzajBbC9rpxaGJWRiDWPjA6g7F71PNhezhxHpZycOLgp4wjE9jxQOycpG5/7yWQ2Aur1V845mzdv7pT5XQTqjD/PP/981YbXcebMmVUbftZ33313p5w5Ch7wF9+YJvHGN6ZBvPGNaRBvfGMaZKDiXkT0jSbKpGFmZwigPjJLiTDsIHLVVVdVbfhIrylTplRtOHW3Eu2UCMXijYq8Y6FKCZCZCCwWeVTEGjtGqfEoUY6foYpY4+uUAJgRIDmiUUWssWNUJoJP3Uuda8/rePXVV1dt+Jlljr7KHM3GcwdQOcA9+OCDnbJyQlL4i29Mg3jjG9Mg3vjGNMhAbfxSSmX7sq2j7CN2tlBtVq5c2SkrmzaT3YYdLVTmGg4UUc4g6qhmDsJYs2ZN1Sbj0MR2prJX2TZVtjE7gyhHJGXjc53qm1FteIzqeaxbt65TVhrQggULOmVlq3NgkzpGXQUpcTvlIMPPP3MkeGbN1FyXLl3aKbODkQp0UviLb0yDeOMb0yDe+MY0iDe+MQ0yUHHvxIkTlaDEAsasWbOq65QTDcPHYylRbNmyZZ0yR+sBtTD05ptvVm34DEAVCajEGzU3ZuPGjZ2yOlKM76fELBbgMmfYKycbJW4ySszKOBmxKKnETh7jnDn1ocwcxabmyvNQmXyUuMlidMYRScFORUqQZZFQjYfTpmezPzH+4hvTIN74xjSIN74xDeKNb0yDDFTcGxoaqkQVFkuUUMbilWrTr18glzKKRRgVIcWCikqnrODrrrzyyqoNi1A7duyo2rC3ljpvkMXFjEipUnjt3LmzquPnkYk043sB9Vqr58Fp1pRAykKiis5jjzslWrLXJpCL/OP7K2GT3z0W6VQb5TXJ41YCYAZ/8Y1pEG98YxrEG9+YBhl4dB7bcWwLqqg2Tl+sYPtZ2atcpyKt2F5VUX4cZadsXBWxl7Ezp0+f3imriDG2u3k8ALBw4cJO+YorrqjasH6h1l7ZkByxpmzaTOpydrpSzjl8LzVGdnJSKclZK1D2s5qrev4MP9eMvqS0At4LmSi/0eIvvjEN4o1vTIOkN35EDEXEryPiyV55YUQ8HxEbIuInEdE/s4AxZlzwfr743wQwMr3s9wH8oJRyLYB9AO46kwMzxpw9UuJeRMwFcAuA/wngL2I4HGkZgDt6TZYD+C6Avz1VPxMmTKhEHxY5VDQai3Lq3HB2WOEz+oA6Yk+JaxxppQQWvk4JcCq9dyaKi+ehBEgW6pS49+qrr3bKs2fP7ntvFdWmosj4mSmhiue/YcOGqg3PQ6XDyjhmseClItZ4HVX6cyWc8Zqod4avU/2w4Jh5F5RAzKJkxilNkf3i/w2AvwTw3hOeBmB/KeW9u24FUEuyxphxSd+NHxF/DGBnKeWFkdWiqfynJiLujohVEbFKfc2NMYMn86P+JwH8SUR8AcAFAC7F8E8AkyPivN5Xfy6AberiUsoDAB4AgHnz5uV+DjHGnFX6bvxSyncAfAcAIuImAN8qpfxpRPwDgC8DeAzAnQAe79fXRRddhOuuu65Tx8dh7du3r++gMxlW+Ox1oA4UUfYQ21DKXuQ6NR5lm7Ptp2w41gtUdh226VUgEd9r7dq1VRu2w5WzSmZuSgfhNVIORKxnZI4LU845/ByVLsE2vRqz0iq4b+X4kzkGjvvOZPtR716/fs60ja/4NoaFvo0Ytvl/dBp9GWMGyPty2S2lPAPgmd6fNwG4/swPyRhztrHnnjEN4o1vTIMMNDrv7bffxiuvvNKpY0FJCTyLFy/ulFkgBIB77723U1bi2v33398p33DDDVWb2267rVNW542zKKVEOpW+mfvKnKOu5sGCn8oAxCKPcjxhp56Mc4wiE4mYScGtUpJnxDUWxVRqdY58U20yqLlmnL543GoeLNype/VrMwhxzxhzjuKNb0yDeOMb0yADtfGPHTtWZchlm4VtMQB45plnOuXVq1dXbdjZQWkFfLa4CqRhe1HZWYyy6ZQzDDuWqDYZG5/tOM7aA9R6gnIq4X6U5qCcSFRf/a5TTjWsg6j78/PIBNco1/DMEVYKnquyoXmuyhEok1GY3zWlefB4bOMbY9J44xvTIN74xjSIN74xDTLw9Nr9zhtXQtnDDz98ymuAWhRUbVgoU0dxZWDnCyUUZdIyK5Es4zDDYpISxTgNtRLp+Flkowz5fkrM4jXJpNvOOOcoRyQWCdVz5eehxpwRchWqL4bnoa7hTFNqzXgeGfFR4S++MQ3ijW9Mg3jjG9MgA7XxI6Ky0R566KFOOWP7KFuU7U5l07J9pI5c5n6UjctZYJRdnnFGUfD8VSYhPs55tEc/MWquyu7ltVVrzVqNOtKMj4pW2XV4HkoD4nVV68Hrmp0r28yqbx5Tpk0my64aY8Z5KoO/+MY0iDe+MQ3ijW9Mg3jjG9MgAxX3du3ahR/+8IedOhY5lMCSEfemTJnSKW/evLlq84lPfKJTVtl1WJhRIgzXqYjCTBpmJWTy3JRwxvdTY8xEPXKbrCCZmQeLuMrJidcokzo7M1cF95MRJIH6fcik5R5tdF5GyOR+VJsM/uIb0yDe+MY0iDe+MQ0yUBv/+PHjOHjwYKeObZSMnaWOpf7oRz/aKd98881VG9YBHn300VMPGDlHC2XPK5uaM6qouWay9LBjh7Kf+SiyTMZWlfElE9yjAmdGY9Mqh5XMkWZ8XSYTruon46ykxpiB7585kjtzL26TeV8Bf/GNaRJvfGMaxBvfmAbxxjemQSKbseOM3CxiF4AtAKYDqL1nxjfn4piBc3PcHvPomV9KmdGv0UA3/n/cNGJVKWXJwG98GpyLYwbOzXF7zGcf/6hvTIN44xvTIGO18R8Yo/ueDufimIFzc9we81lmTGx8Y8zY4h/1jWmQgW/8iLg5Il6OiI0Rcd+g758hIh6MiJ0RsXZE3dSIWBERG3q/TzlVH4MmIq6MiF9ExPqIWBcR3+zVj9txR8QFEbEyIl7sjfmvevULI+L53ph/EhF1EMEYExFDEfHriHiyVx73Yx7JQDd+RAwB+F8A/gjAYgBfjYjFgxxDkr8DwFE+9wF4upRyLYCne+XxxDEA95ZSFgG4EcB/7a3teB73OwCWlVL+E4DrANwcETcC+D6AH/TGvA/AXWM4xpPxTQDrR5TPhTH/B4P+4l8PYGMpZVMp5V0AjwG4dcBj6Esp5V8B7KXqWwEs7/15OYAvDnRQfSilbC+lrO79+RCGX8o5GMfjLsO8d5j9xN7loDxeAAAB60lEQVSvAmAZgH/s1Y+rMQNARMwFcAuA/9MrB8b5mJlBb/w5AN4YUd7aqzsXmFlK2Q4MbzIAl4/xeE5KRCwA8HEAz2Ocj7v3I/MaADsBrADwKoD9pZT3YmTH4zvyNwD+EsB7scfTMP7H3GHQG18FC/u/Fc4gEXEJgH8C8OellIP92o81pZTjpZTrAMzF8E+Ei1SzwY7q5ETEHwPYWUp5YWS1aDpuxqwYaCIODP9LeOWI8lwA2wY8htGyIyJmlVK2R8QsDH+hxhURMRHDm/6RUso/96rH/bgBoJSyPyKewbA+MTkizut9QcfbO/JJAH8SEV8AcAGASzH8E8B4HnPFoL/4vwJwbU8B/QCA2wE8MeAxjJYnANzZ+/OdAB4fw7FU9OzMHwFYX0r56xF/NW7HHREzImJy788XAvhDDGsTvwDw5V6zcTXmUsp3SilzSykLMPz+/r9Syp9iHI9ZUkoZ6C8AXwDwCoZtuf8+6Psnx/gogO0AjmL4p5S7MGzHPQ1gQ+/3qWM9Thrzf8bwj5cvAVjT+/WF8TxuAB8D8OvemNcCuL9XfxWAlQA2AvgHAOeP9VhPMv6bADx5Lo35vV/23DOmQey5Z0yDeOMb0yDe+MY0iDe+MQ3ijW9Mg3jjG9Mg3vjGNIg3vjEN8v8BoLoG8O9iap0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "img = image.load_img(\"C:/Users/mike/Pictures/Camera Roll/happym1.jpg\", grayscale=True, target_size=(48, 48))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis = 0)\n",
    "x /= 255\n",
    "custom = model.predict(x)\n",
    "emotion_analysis(custom[0])\n",
    "x = np.array(x, 'float32')\n",
    "x = x.reshape([48, 48]);\n",
    "plt.gray()\n",
    "plt.imshow(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.4) C:\\projects\\opencv-python\\opencv\\modules\\core\\src\\ocl.cpp:5410: error: (-220:Unknown error code -220) OpenCL error CL_OUT_OF_RESOURCES (-5) during call: clEnqueueWriteBuffer(q, handle=00000200AFFC9690, CL_TRUE, offset=0, sz=186432, data=00000200AF871020, 0, 0, 0) in function 'cv::ocl::OpenCLAllocator::upload'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-20740b19f9da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/mike/Pictures/Camera Roll/sad4.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mfaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_cascade\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfaces\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(3.4.4) C:\\projects\\opencv-python\\opencv\\modules\\core\\src\\ocl.cpp:5410: error: (-220:Unknown error code -220) OpenCL error CL_OUT_OF_RESOURCES (-5) during call: clEnqueueWriteBuffer(q, handle=00000200AFFC9690, CL_TRUE, offset=0, sz=186432, data=00000200AF871020, 0, 0, 0) in function 'cv::ocl::OpenCLAllocator::upload'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "face_cascade = cv2.CascadeClassifier('D:/Programme/Anaconda3/envs/compx/Library/etc/haarcascades/haarcascade_frontalface_default.xml')\n",
    "img = cv2.imread('C:/Users/mike/Pictures/Camera Roll/sad4.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    cv2.imshow('img',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
